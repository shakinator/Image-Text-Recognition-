{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image text recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image as pilImg\n",
    "import os \n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing Train Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering images with labels of length>=4 and length <=12 for this Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_image_names(file_path,number):\n",
    "    \"\"\"\n",
    "    Takes the file path of images annotation txt file with the number of images names to be extracted\n",
    "    and returns the list of file names having label length <=12\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        file_names=f.readlines()\n",
    "        f.close()\n",
    "        count=0\n",
    "        img_names=[]\n",
    "        for file in file_names:\n",
    "            _,label,_=file.split('_')\n",
    "            if len(label)>=4 and len(label)<=12:\n",
    "                img_names.append(file)\n",
    "                count+=1\n",
    "            if count==number:\n",
    "                break\n",
    "        images_names=['mnt/ramdisk/max/90kDICT32px'+x.strip() for x in img_names]\n",
    "        return images_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images= Extract_image_names('mnt/ramdisk/max/90kDICT32px/annotation_train.txt',100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mnt/ramdisk/max/90kDICT32px./2425/1/115_Lube_45484.jpg 45484',\n",
       " 'mnt/ramdisk/max/90kDICT32px./2425/1/114_Spencerian_73323.jpg 73323',\n",
       " 'mnt/ramdisk/max/90kDICT32px./2425/1/112_CARPENTER_11682.jpg 11682',\n",
       " 'mnt/ramdisk/max/90kDICT32px./2425/1/110_savannas_67969.jpg 67969',\n",
       " 'mnt/ramdisk/max/90kDICT32px./2425/1/109_unfix_82473.jpg 82473']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file_names(file_names):\n",
    "    clean_files=[]\n",
    "    for file in file_names:\n",
    "        main_folder,img_loc,extension=file.split('.')\n",
    "        #Removing the image number at the end\n",
    "        extension,_=extension.split(' ')\n",
    "        img_file=main_folder+img_loc+'.'+extension\n",
    "        clean_files.append(img_file)\n",
    "    return clean_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned=clean_file_names(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mnt/ramdisk/max/90kDICT32px/2425/1/115_Lube_45484.jpg',\n",
       " 'mnt/ramdisk/max/90kDICT32px/2425/1/114_Spencerian_73323.jpg',\n",
       " 'mnt/ramdisk/max/90kDICT32px/2425/1/112_CARPENTER_11682.jpg',\n",
       " 'mnt/ramdisk/max/90kDICT32px/2425/1/110_savannas_67969.jpg',\n",
       " 'mnt/ramdisk/max/90kDICT32px/2425/1/109_unfix_82473.jpg']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cleaned[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.DataFrame({'ImageName':train_cleaned})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2425/1/115_Lube_45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2425/1/114_Spencer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2425/1/112_CARPENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2425/1/110_savanna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2425/1/109_unfix_8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ImageName\n",
       "0  mnt/ramdisk/max/90kDICT32px/2425/1/115_Lube_45...\n",
       "1  mnt/ramdisk/max/90kDICT32px/2425/1/114_Spencer...\n",
       "2  mnt/ramdisk/max/90kDICT32px/2425/1/112_CARPENT...\n",
       "3  mnt/ramdisk/max/90kDICT32px/2425/1/110_savanna...\n",
       "4  mnt/ramdisk/max/90kDICT32px/2425/1/109_unfix_8..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Ground Truth Values are present in the image file names, so we need to extract it and Store it in Ground Truth Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ground_truth(files):\n",
    "    \"\"\"\n",
    "    Given the file names of images, extracts the Ground Truth Values and returns a list of Ground Truth Labels in All Capitals\n",
    "    \"\"\"\n",
    "    txt_labels=[]\n",
    "    for file in files:\n",
    "        folder,ground_truth,image=file.split('_')\n",
    "        ground_truth=ground_truth.upper()\n",
    "        txt_labels.append(ground_truth)\n",
    "    return txt_labels        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_ground_truths=extract_ground_truth(train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Labels']=Train_ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2425/1/115_Lube_45...</td>\n",
       "      <td>LUBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2425/1/114_Spencer...</td>\n",
       "      <td>SPENCERIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2425/1/112_CARPENT...</td>\n",
       "      <td>CARPENTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2425/1/110_savanna...</td>\n",
       "      <td>SAVANNAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2425/1/109_unfix_8...</td>\n",
       "      <td>UNFIX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ImageName      Labels\n",
       "0  mnt/ramdisk/max/90kDICT32px/2425/1/115_Lube_45...        LUBE\n",
       "1  mnt/ramdisk/max/90kDICT32px/2425/1/114_Spencer...  SPENCERIAN\n",
       "2  mnt/ramdisk/max/90kDICT32px/2425/1/112_CARPENT...   CARPENTER\n",
       "3  mnt/ramdisk/max/90kDICT32px/2425/1/110_savanna...    SAVANNAS\n",
       "4  mnt/ramdisk/max/90kDICT32px/2425/1/109_unfix_8...       UNFIX"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('Train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing Validation Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation_images=Extract_image_names('mnt/ramdisk/max/90kDICT32px/annotation_val.txt',12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cleaned=clean_file_names(Validation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=pd.DataFrame({'ImageName':val_cleaned})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2697/6/466_MONIKER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2697/6/464_FIRESTO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2697/6/462_Repurch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2697/6/461_PIGTAIL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2697/6/460_landlad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ImageName\n",
       "0  mnt/ramdisk/max/90kDICT32px/2697/6/466_MONIKER...\n",
       "1  mnt/ramdisk/max/90kDICT32px/2697/6/464_FIRESTO...\n",
       "2  mnt/ramdisk/max/90kDICT32px/2697/6/462_Repurch...\n",
       "3  mnt/ramdisk/max/90kDICT32px/2697/6/461_PIGTAIL...\n",
       "4  mnt/ramdisk/max/90kDICT32px/2697/6/460_landlad..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Val_ground_truths=extract_ground_truth(val_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['Labels']=Val_ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2697/6/466_MONIKER...</td>\n",
       "      <td>MONIKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2697/6/464_FIRESTO...</td>\n",
       "      <td>FIRESTORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2697/6/462_Repurch...</td>\n",
       "      <td>REPURCHASES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2697/6/461_PIGTAIL...</td>\n",
       "      <td>PIGTAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/2697/6/460_landlad...</td>\n",
       "      <td>LANDLADIES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ImageName       Labels\n",
       "0  mnt/ramdisk/max/90kDICT32px/2697/6/466_MONIKER...      MONIKER\n",
       "1  mnt/ramdisk/max/90kDICT32px/2697/6/464_FIRESTO...    FIRESTORM\n",
       "2  mnt/ramdisk/max/90kDICT32px/2697/6/462_Repurch...  REPURCHASES\n",
       "3  mnt/ramdisk/max/90kDICT32px/2697/6/461_PIGTAIL...      PIGTAIL\n",
       "4  mnt/ramdisk/max/90kDICT32px/2697/6/460_landlad...   LANDLADIES"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.to_csv('Validation_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing Test Data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images=Extract_image_names('mnt/ramdisk/max/90kDICT32px/annotation_test.txt',15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned=clean_file_names(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.DataFrame({'ImageName':test_cleaned})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/3000/7/182_slinkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/3000/7/181_REMODEL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/3000/7/180_Chronog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/3000/7/179_Impeach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/3000/7/177_Loots_4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ImageName\n",
       "0  mnt/ramdisk/max/90kDICT32px/3000/7/182_slinkin...\n",
       "1  mnt/ramdisk/max/90kDICT32px/3000/7/181_REMODEL...\n",
       "2  mnt/ramdisk/max/90kDICT32px/3000/7/180_Chronog...\n",
       "3  mnt/ramdisk/max/90kDICT32px/3000/7/179_Impeach...\n",
       "4  mnt/ramdisk/max/90kDICT32px/3000/7/177_Loots_4..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ground_truths=extract_ground_truth(test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Labels']=test_ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/3000/7/182_slinkin...</td>\n",
       "      <td>SLINKING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/3000/7/181_REMODEL...</td>\n",
       "      <td>REMODELERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/3000/7/180_Chronog...</td>\n",
       "      <td>CHRONOGRAPHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/3000/7/179_Impeach...</td>\n",
       "      <td>IMPEACHING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mnt/ramdisk/max/90kDICT32px/3000/7/177_Loots_4...</td>\n",
       "      <td>LOOTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ImageName        Labels\n",
       "0  mnt/ramdisk/max/90kDICT32px/3000/7/182_slinkin...      SLINKING\n",
       "1  mnt/ramdisk/max/90kDICT32px/3000/7/181_REMODEL...    REMODELERS\n",
       "2  mnt/ramdisk/max/90kDICT32px/3000/7/180_Chronog...  CHRONOGRAPHS\n",
       "3  mnt/ramdisk/max/90kDICT32px/3000/7/179_Impeach...    IMPEACHING\n",
       "4  mnt/ramdisk/max/90kDICT32px/3000/7/177_Loots_4...         LOOTS"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('Test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image Processing -Converting to Single Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to Extract The images present in multiple folder paths in the original Big Data set and store them in a specified Train and Validation Data folder by taking the single channel gray scale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_store_single_channel(destination_folder,files):\n",
    "    \"\"\"\n",
    "    Takes the images in a folder, distination folder path and \n",
    "    converts the image to single channel gray scale,\n",
    "    stores the image in the destination folder and returns image destination list\n",
    "    \"\"\"\n",
    "    start=datetime.now()\n",
    "    destination_list=[]\n",
    "    count=1\n",
    "    for file in files:\n",
    "        #Removing the extra folder structures\n",
    "        _,_,_,_,_,_,Name=file.split('/')\n",
    "        _,img,_=Name.split('_')\n",
    "        destination=destination_folder+str(count)+'_'+img+'.jpg'\n",
    "        cv_img=cv2.imread(file)\n",
    "        #So extracting image from any 1 channel gives a single channel Grayscale image\n",
    "        cv_img_sc=cv_img[:,:,1]\n",
    "        cv2.imwrite(destination,cv_img_sc)\n",
    "        destination_list.append(destination)\n",
    "        count+=1\n",
    "\n",
    "    print('Time Taken for Processing: ',datetime.now() - start)\n",
    "    return destination_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing Train Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('Train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files=train_data['ImageName'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Processing:  0:08:21.303102\n"
     ]
    }
   ],
   "source": [
    "train_dest=img_store_single_channel('Train_data/',train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating Train Dataframe with new destination file paths\n",
    "train_data['ImageName']=train_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_data/1_Lube.jpg</td>\n",
       "      <td>LUBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_data/2_Spencerian.jpg</td>\n",
       "      <td>SPENCERIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_data/3_CARPENTER.jpg</td>\n",
       "      <td>CARPENTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_data/4_savannas.jpg</td>\n",
       "      <td>SAVANNAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_data/5_unfix.jpg</td>\n",
       "      <td>UNFIX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ImageName      Labels\n",
       "0        Train_data/1_Lube.jpg        LUBE\n",
       "1  Train_data/2_Spencerian.jpg  SPENCERIAN\n",
       "2   Train_data/3_CARPENTER.jpg   CARPENTER\n",
       "3    Train_data/4_savannas.jpg    SAVANNAS\n",
       "4       Train_data/5_unfix.jpg       UNFIX"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('Train_Final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing Validation Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=pd.read_csv('Validation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files=val_data['ImageName'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Processing:  0:00:47.942159\n"
     ]
    }
   ],
   "source": [
    "val_dest=img_store_single_channel('Val_data/',val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating Validation Dataframe with new destination file paths\n",
    "val_data['ImageName']=val_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Val_data/1_MONIKER.jpg</td>\n",
       "      <td>MONIKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Val_data/2_FIRESTORM.jpg</td>\n",
       "      <td>FIRESTORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Val_data/3_Repurchases.jpg</td>\n",
       "      <td>REPURCHASES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Val_data/4_PIGTAIL.jpg</td>\n",
       "      <td>PIGTAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Val_data/5_landladies.jpg</td>\n",
       "      <td>LANDLADIES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ImageName       Labels\n",
       "0      Val_data/1_MONIKER.jpg      MONIKER\n",
       "1    Val_data/2_FIRESTORM.jpg    FIRESTORM\n",
       "2  Val_data/3_Repurchases.jpg  REPURCHASES\n",
       "3      Val_data/4_PIGTAIL.jpg      PIGTAIL\n",
       "4   Val_data/5_landladies.jpg   LANDLADIES"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the updated Validation Dataframe\n",
    "val_data.to_csv('Validation_Final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('Test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files=test_data['ImageName'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Processing:  0:01:54.144477\n"
     ]
    }
   ],
   "source": [
    "test_dest=img_store_single_channel('Test_data/',test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating Test Dataframe with new destination file paths\n",
    "test_data['ImageName']=test_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_data/1_slinking.jpg</td>\n",
       "      <td>SLINKING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_data/2_REMODELERS.jpg</td>\n",
       "      <td>REMODELERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_data/3_Chronographs.jpg</td>\n",
       "      <td>CHRONOGRAPHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_data/4_Impeaching.jpg</td>\n",
       "      <td>IMPEACHING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_data/5_Loots.jpg</td>\n",
       "      <td>LOOTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ImageName        Labels\n",
       "0      Test_data/1_slinking.jpg      SLINKING\n",
       "1    Test_data/2_REMODELERS.jpg    REMODELERS\n",
       "2  Test_data/3_Chronographs.jpg  CHRONOGRAPHS\n",
       "3    Test_data/4_Impeaching.jpg    IMPEACHING\n",
       "4         Test_data/5_Loots.jpg         LOOTS"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the updated Test Dataframe\n",
    "test_data.to_csv('Test_Final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Image Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('Train_data.csv')\n",
    "val_data=pd.read_csv('Validation_Data.csv')\n",
    "test_data=pd.read_csv('Test_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Write_Image_Sizes(filenames,storage_file):\n",
    "    \"\"\"\n",
    "    Takes the File names, writes the width and height of images in csv along with file names\n",
    "    \"\"\"\n",
    "    store_file=open(storage_file,'w+')\n",
    "    store_file.write(\"ImageName,Height,Width\")\n",
    "    store_file.write(\"\\n\")\n",
    "    cnt=0\n",
    "    for file in filenames:\n",
    "        cv_img=cv2.imread(file)\n",
    "        #img.shape gives (img_height,img_width,img_channel)\n",
    "        store_file.write(str(file)+\",\"+str(cv_img.shape[0])+\",\"+str(cv_img.shape[1]))\n",
    "        store_file.write(\"\\n\")\n",
    "        cnt+=1\n",
    "        if cnt%10000==0:\n",
    "            print(\"Processed Images: \",cnt)\n",
    "    store_file.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_names=list(train_data['ImageName'].values)\n",
    "val_image_names=list(val_data['ImageName'].values)\n",
    "test_image_names=list(test_data['ImageName'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Images:  10000\n",
      "Processed Images:  20000\n",
      "Processed Images:  30000\n",
      "Processed Images:  40000\n",
      "Processed Images:  50000\n",
      "Processed Images:  60000\n",
      "Processed Images:  70000\n",
      "Processed Images:  80000\n",
      "Processed Images:  90000\n",
      "Processed Images:  100000\n"
     ]
    }
   ],
   "source": [
    "Write_Image_Sizes(train_image_names,'Train_image_sizes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Images:  10000\n"
     ]
    }
   ],
   "source": [
    "Write_Image_Sizes(val_image_names,'Validation_image_sizes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Images:  10000\n"
     ]
    }
   ],
   "source": [
    "Write_Image_Sizes(test_image_names,'Test_image_sizes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_size=pd.read_csv('Train_image_sizes.csv')\n",
    "val_img_size=pd.read_csv('Validation_image_sizes.csv')\n",
    "test_img_size=pd.read_csv('Test_image_sizes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.036070</td>\n",
       "      <td>115.635930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.359514</td>\n",
       "      <td>39.727547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>109.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>136.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>608.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Height          Width\n",
       "count  100000.000000  100000.000000\n",
       "mean       31.036070     115.635930\n",
       "std         0.359514      39.727547\n",
       "min         9.000000       1.000000\n",
       "25%        31.000000      88.000000\n",
       "50%        31.000000     109.000000\n",
       "75%        31.000000     136.000000\n",
       "max        32.000000     608.000000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_size.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12000.000000</td>\n",
       "      <td>12000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.037083</td>\n",
       "      <td>115.644167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.326421</td>\n",
       "      <td>39.476965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>136.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>440.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Height         Width\n",
       "count  12000.000000  12000.000000\n",
       "mean      31.037083    115.644167\n",
       "std        0.326421     39.476965\n",
       "min       16.000000     25.000000\n",
       "25%       31.000000     88.000000\n",
       "50%       31.000000    110.000000\n",
       "75%       31.000000    136.000000\n",
       "max       32.000000    440.000000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_img_size.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15000.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.040600</td>\n",
       "      <td>115.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.353497</td>\n",
       "      <td>39.400603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>109.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>136.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>464.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Height         Width\n",
       "count  15000.000000  15000.000000\n",
       "mean      31.040600    115.166000\n",
       "std        0.353497     39.400603\n",
       "min        6.000000      1.000000\n",
       "25%       31.000000     87.000000\n",
       "50%       31.000000    109.000000\n",
       "75%       31.000000    136.000000\n",
       "max       32.000000    464.000000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_size.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Images in Train Data have a mean height of 31 and mean width of ~116\n",
    "2. Almost 75% of Images have width 136 and height 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "def hasDigits(inputText):\n",
    "    \"\"\"\n",
    "    Returns True if the given input text has digits in it otherwise returns False\n",
    "    \"\"\"\n",
    "    return bool(re.search(r'\\d', inputText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_count(labels,size):\n",
    "    \"\"\"\n",
    "    Takes the list of labels and counts the number of labels with and without digits\n",
    "    and prints its percentage\n",
    "    \"\"\"\n",
    "    present=0\n",
    "    absent=0\n",
    "    for i in labels:\n",
    "        pres=hasDigits(i)\n",
    "        if pres==True:\n",
    "            present+=1\n",
    "        else:\n",
    "            absent+=1\n",
    "    present_percent=(present/size)*100\n",
    "    absent_percent=(absent/size)*100\n",
    "    print('Labels with Digits: ',present_percent,' %')\n",
    "    print('Labels without Digits: ',absent_percent,' %')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1. Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('Train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_len=[len(str(x)) for x in train_data['Labels'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the updated Train Dataframe\n",
    "train_data.to_csv('Train_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_len_dict=Counter(train_label_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 4612,\n",
       "         10: 11943,\n",
       "         9: 15156,\n",
       "         8: 17246,\n",
       "         5: 8326,\n",
       "         6: 12809,\n",
       "         12: 4963,\n",
       "         7: 16908,\n",
       "         11: 8036,\n",
       "         3: 1})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing Value counts of Label Lengths \n",
    "train_label_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys = [k for k in sorted(train_label_len_dict, key=train_label_len_dict.get, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking Top 5 keys with highest values \n",
    "train_top_keys=train_keys[:5]\n",
    "train_top_values=[train_label_len_dict.get(k) for k in train_top_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels with Digits:  0.078  %\n",
      "Labels without Digits:  99.922  %\n"
     ]
    }
   ],
   "source": [
    "train_labels=[str(x) for x in train_data['Labels'].values]\n",
    "digit_count(train_labels,len(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2. Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=pd.read_csv('Validation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_len=[len(str(x)) for x in val_data['Labels'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_len_dict=Counter(val_label_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({7: 2078,\n",
       "         9: 1817,\n",
       "         11: 942,\n",
       "         10: 1525,\n",
       "         8: 1944,\n",
       "         5: 1002,\n",
       "         12: 619,\n",
       "         6: 1584,\n",
       "         4: 489})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing Value counts of Label Lengths \n",
    "val_label_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_keys = [k for k in sorted(val_label_len_dict, key=val_label_len_dict.get, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking Top 5 keys with highest values \n",
    "val_top_keys=val_keys[:5]\n",
    "val_top_values=[val_label_len_dict.get(k) for k in val_top_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Data Digit Presence Percentage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels with Digits:  0.09166666666666666  %\n",
      "Labels without Digits:  99.90833333333333  %\n"
     ]
    }
   ],
   "source": [
    "val_labels=[str(x) for x in val_data['Labels'].values]\n",
    "digit_count(val_labels,len(val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.3. Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('Test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_len=[len(str(x)) for x in test_data['Labels'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_len_dict=Counter(test_label_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({8: 2490,\n",
       "         10: 1742,\n",
       "         12: 730,\n",
       "         5: 1317,\n",
       "         9: 2271,\n",
       "         11: 1255,\n",
       "         7: 2495,\n",
       "         6: 1990,\n",
       "         4: 710})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing Value counts of Label Lengths \n",
    "test_label_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keys = [k for k in sorted(test_label_len_dict, key=test_label_len_dict.get, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking Top 5 keys with highest values \n",
    "test_top_keys=test_keys[:5]\n",
    "test_top_values=[test_label_len_dict.get(k) for k in test_top_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Data Digit Presence Percentage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels with Digits:  0.08666666666666667  %\n",
      "Labels without Digits:  99.91333333333333  %\n"
     ]
    }
   ],
   "source": [
    "test_labels=[str(x) for x in test_data['Labels'].values]\n",
    "digit_count(test_labels,len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label Letters Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('Train_Final.csv')\n",
    "val_data=pd.read_csv('Validation_Final.csv')\n",
    "test_data=pd.read_csv('Test_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters='ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "train_labels_combined=' '.join([str(x) for x in train_data['Labels'].values])\n",
    "val_labels_combined=' '.join([str(x) for x in val_data['Labels'].values])\n",
    "test_labels_combined=' '.join([str(x) for x in test_data['Labels'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letters_list(letter,combined_labels):\n",
    "    \"\"\"\n",
    "    Takes the letters, combined labels text checks for the presence of each letter and returns a list of letters \n",
    "    present in the combined labels\n",
    "    \"\"\"\n",
    "    letter_list=[]\n",
    "    for i in range(len(letter)):\n",
    "        if letter[i] in combined_labels:\n",
    "            letter_list.append(letter[i])\n",
    "    return letter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_letter_set=set(letters_list(letters,train_labels_combined))\n",
    "val_letter_set=set(letters_list(letters,val_labels_combined))\n",
    "test_letter_set=set(letters_list(letters,test_labels_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Letters in Train Data:  26\n",
      "Number of unique Letters in Validation Data:  26\n",
      "Number of unique Letters in Test Data:  26\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique Letters in Train Data: \",len(train_letter_set))\n",
    "print(\"Number of unique Letters in Validation Data: \",len(val_letter_set))\n",
    "print(\"Number of unique Letters in Test Data: \",len(test_letter_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "All Letters in the alphabet are present in each of Train, Validation and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import random\n",
    "from keras import backend as K\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Letters present in the Label Text\n",
    "letters= '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image height\n",
    "img_h=32\n",
    "#image width\n",
    "img_w=170\n",
    "#image Channels\n",
    "img_c=1\n",
    "num_classes=len(letters)+1\n",
    "batch_size=32\n",
    "max_length=15 # considering max length of ground truths labels to be 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words_labels(word):\n",
    "    \"\"\"\n",
    "    Encodes the Ground Truth Labels to a list of Values like eg.HAT returns [17,10,29]\n",
    "    \"\"\"\n",
    "    label_lst=[]\n",
    "    for char in word:\n",
    "        label_lst.append(letters.find(char)) \n",
    "    return label_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_from_labels(labels):\n",
    "    \"\"\"\n",
    "    converts the list of encoded integer labels to word strings like eg. [12,10,29] returns CAT \n",
    "    \"\"\"\n",
    "    txt=[]\n",
    "    for ele in labels:\n",
    "        if ele == len(letters): # CTC blank space\n",
    "            txt.append(\"\")\n",
    "        else:\n",
    "            #print(letters[ele])\n",
    "            txt.append(letters[ele])\n",
    "    return \"\".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_function(args):\n",
    "    \"\"\"\n",
    "    CTC loss function takes the values passed from the model returns the CTC loss using Keras Backend ctc_batch_cost function\n",
    "    \"\"\"\n",
    "    y_pred, y_true, input_length, label_length = args \n",
    "   \n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(y_true, y_pred, input_length, label_length)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, img_dirpath, img_w, img_h,\n",
    "                 batch_size,n,output_labels,max_text_len=15):\n",
    "        self.img_h = img_h                    #Image Height\n",
    "        self.img_w = img_w                    #Image Width\n",
    "        self.batch_size = batch_size          #Batch size of Input\n",
    "        self.max_text_len = max_text_len      #Maximum Text length of Labels\n",
    "        \n",
    "#         self.n =len(self.img_dir)                           #Number of images in img_matrix\n",
    "        self.n=n\n",
    "        self.img_dir = img_dirpath[:self.n]     # images list\n",
    "        self.indexes = list(range(self.n))   #List of indices for each image in img_matrix\n",
    "        self.cur_index = 0                   #Current index which points to image being loaded \n",
    "        self.imgs = np.zeros((self.n, self.img_h, self.img_w))\n",
    "        self.texts =  output_labels[:self.n]                  #List of Ground Truth Label texts\n",
    "\n",
    "   \n",
    "    def build_data(self):\n",
    "        \"\"\"\n",
    "        Build The Image Data\n",
    "        \"\"\"\n",
    "        print(self.n, \" Image Loading start...\")\n",
    "        for i, img_file in enumerate(self.img_dir):\n",
    "            img = cv2.imread(img_file)\n",
    "            img = img[:,:,1]                               #Extracting Single Channel Image\n",
    "            img = cv2.resize(img, (self.img_w, self.img_h))\n",
    "            img = img /255\n",
    "            self.imgs[i, :, :]= img\n",
    "            if i%10000==0:\n",
    "                print(\"Loaded Images: \",i)\n",
    "           \n",
    "        print(\"Number of Texts matches with Total Number of Images :\",len(self.texts) == self.n)\n",
    "        print(self.n, \" Image Loading finish...\")\n",
    "\n",
    "\n",
    "    def next_data(self): \n",
    "        \"\"\"\n",
    "        Returns image and text data pointed by the current index\n",
    "        \"\"\"\n",
    "        self.cur_index += 1\n",
    "        #If current index becomes more than the number of images, make current index 0 \n",
    "        #and shuffle the indices list for random picking of image and text data\n",
    "        if self.cur_index >= self.n:\n",
    "            self.cur_index = 0\n",
    "            random.shuffle(self.indexes)\n",
    "        return self.imgs[self.indexes[self.cur_index]], self.texts[self.indexes[self.cur_index]]\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Creates a batch of images and text data equal to the batch_size,\n",
    "        computes the parameters needed for CTC and returns the inputs to the Model\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])  #Single channel Gray Size Scale images for input\n",
    "            #Initilizing with -1 to aid for padding labels of different lengths\n",
    "            Y_data = np.ones([self.batch_size, self.max_text_len])* -1        #Text labels for input\n",
    "           #input_length for CTC which is the number of time-steps of the RNN output\n",
    "            input_length = np.ones((self.batch_size, 1)) * 40\n",
    "            label_length = np.zeros((self.batch_size, 1))                   #label length for CTC\n",
    "            source_str=[]                                                   #List to store Ground Truth Labels\n",
    "            for i in range(self.batch_size):\n",
    "                img, text = self.next_data() #getting the image and text data pointed by current index\n",
    "                                    #taking transpose of image\n",
    "                img=img.T\n",
    "                img = np.expand_dims(img, -1)  #expanding image to have a single channel\n",
    "                X_data[i] = img\n",
    "                label=encode_words_labels(text) # encoding label text to integer list and storing in temp label variable\n",
    "                lbl_len=len(label)\n",
    "                Y_data[i,0:lbl_len] = label #Storing the label till its length and padding others\n",
    "                label_length[i] = len(label)\n",
    "                source_str.append(text) #storing Ground Truth Labels which will be accessed as reference for calculating metrics\n",
    "            \n",
    "        #Preparing the input for the Model\n",
    "            inputs = {\n",
    "                'img_input': X_data,  \n",
    "                'ground_truth_labels': Y_data,  \n",
    "                'input_length': input_length,  \n",
    "                'label_length': label_length,\n",
    "                'source_str': source_str  # used for visualization only\n",
    "            }\n",
    "            #Preparing output for the Model and intializing to zeros\n",
    "            outputs = {'ctc': np.zeros([self.batch_size])}  \n",
    "            yield (inputs, outputs) # Return the Prepared input and output to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPool2D, Dense,MaxPooling2D\n",
    "from keras.layers import AveragePooling2D, Flatten, Activation, Bidirectional\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.layers import Concatenate, Add, Multiply, Lambda\n",
    "from keras.layers import UpSampling2D, Reshape\n",
    "from keras.layers.merge import add,concatenate\n",
    "from keras.layers import Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1. Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_text_recogniser_model_1(stage,drop_out_rate=0.35):\n",
    "    \"\"\"\n",
    "    Builds the model by taking in the stage variable which specifes the stage,\n",
    "    if the stage is training: model takes inputs required for computing ctc_batch_cost function\n",
    "    else : model takes input as images which is used for prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (1, img_w, img_h)\n",
    "    else:\n",
    "        input_shape = (img_w, img_h, 1)\n",
    "       \n",
    "    model_input=Input(shape=input_shape,name='img_input',dtype='float32')\n",
    "\n",
    "    # Convolution layer \n",
    "    model = Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(model_input) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(2, 2), name='max1')(model) \n",
    "\n",
    "    model = Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(2, 2), name='max2')(model) \n",
    "\n",
    "    model = Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(model)\n",
    "    model=Dropout(drop_out_rate)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(1, 2), name='max3')(model)  \n",
    "\n",
    "    model = Conv2D(512, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(512, (3, 3), padding='same', name='conv6')(model)\n",
    "    model=Dropout(drop_out_rate)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(1, 2), name='max4')(model) \n",
    "\n",
    "    model = Conv2D(512, (2, 2), padding='same', kernel_initializer='he_normal', name='con7')(model)\n",
    "    model=Dropout(0.25)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)    \n",
    "\n",
    "    # CNN to RNN\n",
    "    model = Reshape(target_shape=((42, 1024)), name='reshape')(model)  \n",
    "    model = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(model)  \n",
    "\n",
    "    # RNN layer\n",
    "    model=Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal'), merge_mode='sum')(model)\n",
    "    model=Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal'), merge_mode='concat')(model)\n",
    "\n",
    "    # transforms RNN output to character activations:\n",
    "    model = Dense(num_classes, kernel_initializer='he_normal',name='dense2')(model) \n",
    "    y_pred = Activation('softmax', name='softmax')(model)\n",
    "\n",
    "    \n",
    "    labels = Input(name='ground_truth_labels', shape=[max_length], dtype='float32') \n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64') \n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64') \n",
    "\n",
    "    #CTC loss function\n",
    "    loss_out = Lambda(ctc_loss_function, output_shape=(1,),name='ctc')([y_pred, labels, input_length, label_length]) #(None, 1)\n",
    "\n",
    "    if stage=='train':\n",
    "        return model_input,y_pred,Model(inputs=[model_input, labels, input_length, label_length], outputs=loss_out)\n",
    "    else:\n",
    "        return Model(inputs=[model_input], outputs=y_pred)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input,y_pred,img_text_recog=Image_text_recogniser_model_1('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\saksham garg\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#used for visualization\n",
    "# it is a keras backend function used to capture the model ouputs so that it can be used for decoding and calculating metrics\n",
    "test_func = K.function([model_input], [y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img_input (InputLayer)          (None, 170, 32, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 170, 32, 64)  640         img_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 170, 32, 64)  256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 170, 32, 64)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 85, 16, 64)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 85, 16, 128)  73856       max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 85, 16, 128)  512         conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 85, 16, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 42, 8, 128)   0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 42, 8, 256)   295168      max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 42, 8, 256)   1024        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 42, 8, 256)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 42, 8, 256)   590080      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 42, 8, 256)   0           conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 42, 8, 256)   1024        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 42, 8, 256)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max3 (MaxPooling2D)             (None, 42, 4, 256)   0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv2D)                  (None, 42, 4, 512)   1180160     max3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 42, 4, 512)   2048        conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 42, 4, 512)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 42, 4, 512)   2359808     activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 42, 4, 512)   0           conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 42, 4, 512)   2048        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 42, 4, 512)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max4 (MaxPooling2D)             (None, 42, 2, 512)   0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "con7 (Conv2D)                   (None, 42, 2, 512)   1049088     max4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 42, 2, 512)   0           con7[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 42, 2, 512)   2048        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 42, 2, 512)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 42, 1024)     0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 42, 64)       65600       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 42, 256)      657408      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 42, 512)      1050624     bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 42, 37)       18981       bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 42, 37)       0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ground_truth_labels (InputLayer (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 ground_truth_labels[0][0]        \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,350,373\n",
      "Trainable params: 7,345,893\n",
      "Non-trainable params: 4,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_text_recog.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch(test_func, word_batch):\n",
    "    \"\"\"\n",
    "    Takes the Batch of Predictions and decodes the Predictions by Best Path Decoding and Returns the Output\n",
    "    \"\"\"\n",
    "    out = test_func([word_batch])[0] #returns the predicted output matrix of the model\n",
    "    ret = []\n",
    "    for j in range(out.shape[0]):\n",
    "        out_best = list(np.argmax(out[j, 2:], 1))\n",
    "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "        outstr = words_from_labels(out_best)\n",
    "        ret.append(outstr)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracies(actual_labels,predicted_labels,is_train):\n",
    "    \"\"\"\n",
    "    Takes a List of Actual Outputs, predicted Outputs and returns their accuracy and letter accuracy across\n",
    "    all the labels in the list\n",
    "    \"\"\"\n",
    "    accuracy=0\n",
    "    letter_acc=0\n",
    "    letter_cnt=0\n",
    "    count=0\n",
    "    for i in range(len(actual_labels)):\n",
    "        predicted_output=predicted_labels[i]\n",
    "        actual_output=actual_labels[i]\n",
    "        count+=1\n",
    "        for j in range(min(len(predicted_output),len(actual_output))):\n",
    "            if predicted_output[j]==actual_output[j]:\n",
    "                letter_acc+=1\n",
    "        letter_cnt+=max(len(predicted_output),len(actual_output))\n",
    "        if actual_output==predicted_output:\n",
    "            accuracy+=1\n",
    "    final_accuracy=np.round((accuracy/len(actual_labels))*100,2)\n",
    "    final_letter_acc=np.round((letter_acc/letter_cnt)*100,2)\n",
    "    return final_accuracy,final_letter_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CallBacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizCallback(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    The Custom Callback created for printing the Accuracy and Letter Accuracy Metrics at the End of Each Epoch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, test_func, text_img_gen,is_train,acc_compute_batches):\n",
    "        self.test_func = test_func\n",
    "        self.text_img_gen = text_img_gen\n",
    "        self.is_train=is_train                #used to indicate whether the callback is called to for Train or Validation Data\n",
    "        self.acc_batches=acc_compute_batches  # Number of Batches for which the metrics are computed typically equal to steps/epoch\n",
    "\n",
    "    def show_accuracy_metrics(self,num_batches):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy and letter accuracy for each batch of inputs, \n",
    "        and prints the avarage accuracy and letter accuracy across all the batches\n",
    "        \"\"\"\n",
    "        accuracy=0\n",
    "        letter_accuracy=0\n",
    "        batches_cnt=num_batches\n",
    "        while batches_cnt>0:\n",
    "            word_batch = next(self.text_img_gen)[0]   #Gets the next batch from the Data generator\n",
    "            decoded_res = decode_batch(self.test_func,word_batch['img_input'])\n",
    "            actual_res=word_batch['source_str']\n",
    "            acc,let_acc=accuracies(actual_res,decoded_res,self.is_train)\n",
    "            accuracy+=acc\n",
    "            letter_accuracy+=let_acc\n",
    "            batches_cnt-=1\n",
    "        accuracy=accuracy/num_batches\n",
    "        letter_accuracy=letter_accuracy/num_batches\n",
    "        if self.is_train:\n",
    "            print(\"Train Average Accuracy of \"+str(num_batches)+\" Batches: \",np.round(accuracy,2),\" %\")\n",
    "            print(\"Train Average Letter Accuracy of \"+str(num_batches)+\" Batches: \",np.round(letter_accuracy,2),\" %\")\n",
    "        else:\n",
    "            print(\"Validation Average Accuracy of \"+str(num_batches)+\" Batches: \",np.round(accuracy,2),\" %\")\n",
    "            print(\"Validation Average Letter Accuracy of \"+str(num_batches)+\" Batches: \",np.round(letter_accuracy,2),\" %\")\n",
    "            \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.show_accuracy_metrics(self.acc_batches)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "early_stop=EarlyStopping(monitor='val_loss',patience=2,restore_best_weights=True)\n",
    "model_chk_pt=ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=False,save_weights_only=True,verbose=0, mode='auto', period=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs_127\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11840), started 3 days, 15:30:04 ago. (Use '!kill 11840' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x23342d1c388>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labels Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Train Data Labels\n",
    "Train_labels=[str(x) for x in train_data['Labels'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths=[str(x) for x in train_data['ImageName'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nan_cnt=0\n",
    "train_nan_replaced=False\n",
    "for i in range(len(Train_labels)):\n",
    "    if Train_labels[i]=='nan':\n",
    "        Train_labels[i]='NULL'\n",
    "        train_nan_replaced=True\n",
    "        train_nan_cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was there any NULL values written as Nan in Train Data: True\n",
      "Train Nan count:  1\n"
     ]
    }
   ],
   "source": [
    "print('Was there any NULL values written as Nan in Train Data:',train_nan_replaced)\n",
    "print('Train Nan count: ',train_nan_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Validation Data Labels\n",
    "cv_labels=[str(x) for x in val_data['Labels'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path=[str(x) for x in val_data['ImageName'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_nan_cnt=0\n",
    "val_nan_replaced=False\n",
    "for i in range(len(cv_labels)):\n",
    "    if cv_labels[i]=='nan':\n",
    "        cv_labels[i]='NULL'\n",
    "        val_nan_replaced=True\n",
    "        val_nan_cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was there any NULL values written as Nan : False\n",
      "Validation Nan count:  0\n"
     ]
    }
   ],
   "source": [
    "print('Was there any NULL values written as Nan :',val_nan_replaced)\n",
    "print('Validation Nan count: ',val_nan_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instatiating Data Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gene=DataGenerator(train_paths,img_w, img_h,batch_size,100000,Train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000  Image Loading start...\n",
      "Loaded Images:  0\n",
      "Loaded Images:  10000\n",
      "Loaded Images:  20000\n",
      "Loaded Images:  30000\n",
      "Loaded Images:  40000\n",
      "Loaded Images:  50000\n",
      "Loaded Images:  60000\n",
      "Loaded Images:  70000\n",
      "Loaded Images:  80000\n",
      "Loaded Images:  90000\n",
      "Number of Texts matches with Total Number of Images : True\n",
      "100000  Image Loading finish...\n"
     ]
    }
   ],
   "source": [
    "train_gene.build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_batches=int(train_gene.n / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cb_train = VizCallback( test_func, train_gene.next_batch(),True,train_num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen=DataGenerator(val_path,img_w, img_h,batch_size,12000,cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000  Image Loading start...\n",
      "Loaded Images:  0\n",
      "Loaded Images:  10000\n",
      "Number of Texts matches with Total Number of Images : True\n",
      "12000  Image Loading finish...\n"
     ]
    }
   ],
   "source": [
    "val_gen.build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_num_batches=int(val_gen.n / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cb_val = VizCallback( test_func, val_gen.next_batch(),False,val_num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "adam=optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_recog.load_weights('Best_Img_recog_LSTM_Adam_model_run_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_recog.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.1. Train Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\saksham garg\\anaconda3\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saksham garg\\anaconda3\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/1\n",
      "3125/3125 [==============================] - 18098s 6s/step - loss: 2.6554 - val_loss: 2.0708\n",
      "Train Average Accuracy of 3125 Batches:  69.32  %\n",
      "Train Average Letter Accuracy of 3125 Batches:  86.46  %\n",
      "Validation Average Accuracy of 375 Batches:  67.42  %\n",
      "Validation Average Letter Accuracy of 375 Batches:  85.39  %\n",
      "WARNING:tensorflow:From C:\\Users\\saksham garg\\anaconda3\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2332f63b2c8>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_text_recog.fit_generator(generator=train_gene.next_batch(),\n",
    "                    steps_per_epoch=int(train_gene.n / batch_size),\n",
    "                    epochs=1,\n",
    "                    callbacks=[viz_cb_train,viz_cb_val,train_gene,val_gen,tensorboard_callback,early_stop,model_chk_pt],\n",
    "                    validation_data=val_gen.next_batch(),\n",
    "                    validation_steps=int(val_gen.n / batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_recog.save('Best_Img_recog_LSTM_Adam_model_run_weights2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_recog.save('Img_recog_LSTM_Adam_model_run_3_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Output Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_label(out):\n",
    "    \"\"\"\n",
    "    Takes the predicted ouput matrix from the Model and returns the output text for the image\n",
    "    \"\"\"\n",
    "    # out : (1, 42, 37)\n",
    "    # discarding first 2 outputs of RNN as they tend to be garbage \n",
    "    out_best = list(np.argmax(out[0,2:], axis=1))\n",
    "\n",
    "    out_best = [k for k, g in itertools.groupby(out_best)]  # remove overlap value\n",
    "\n",
    "    outstr=words_from_labels(out_best)\n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2. Test Output Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_output_Prediction(model,test_img_names,test_labels):\n",
    "    \"\"\"\n",
    "    Takes the best model, test data image paths, test data groud truth labels and pre-processes the input image to \n",
    "    appropriate format for the model prediction, takes the predicted output matrix and uses best path decoding to \n",
    "    generate predicted text and compares with ground truth text for the input and ouputs the final accuracy,\n",
    "    letter accuracy and  letter count across the entire test set of images, it also returns list of letter mis-match\n",
    "    count for each test point in the whole test set of images\n",
    "    \"\"\"\n",
    "    start=datetime.now()\n",
    "    accuracy=0\n",
    "    letter_acc=0\n",
    "    letter_cnt=0\n",
    "    count=0\n",
    "    letter_mis_match=[]\n",
    "    for i in range(len(test_labels)):\n",
    "        test_img=cv2.imread(test_img_names[i])\n",
    "        test_img_resized=cv2.resize(test_img,(170,32))\n",
    "        test_image=test_img_resized[:,:,1]\n",
    "        test_image=test_image.T\n",
    "        test_image=np.expand_dims(test_image,axis=-1)\n",
    "        test_image=np.expand_dims(test_image, axis=0)\n",
    "        test_image=test_image/255\n",
    "        model_output=model.predict(test_image)\n",
    "        predicted_output=decode_label(model_output)\n",
    "        actual_output=test_labels[i]\n",
    "        count+=1\n",
    "        mis_match=0\n",
    "        for j in range(min(len(predicted_output),len(actual_output))):\n",
    "            if predicted_output[j]==actual_output[j]:\n",
    "                letter_acc+=1\n",
    "            else:\n",
    "                mis_match+=1\n",
    "        letter_cnt+=max(len(predicted_output),len(actual_output))\n",
    "        letter_mis_match.append(mis_match)\n",
    "        if actual_output==predicted_output:\n",
    "            accuracy+=1\n",
    "        if (count%1000)==0:\n",
    "            print(\"Processed \",count,\" Images\")\n",
    "    print(\"Time Taken for Processing: \",datetime.now()-start)\n",
    "    return accuracy,letter_acc,letter_cnt,letter_mis_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3. Model 1 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Image_text_recogniser_model_1('predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Best_Img_recog_LSTM_Adam_model_run_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synth Text Validation Data Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_names=val_data['ImageName'].values\n",
    "val_labels=val_data['Labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  1000  Images\n",
      "Processed  2000  Images\n",
      "Processed  3000  Images\n",
      "Processed  4000  Images\n",
      "Processed  5000  Images\n",
      "Processed  6000  Images\n",
      "Processed  7000  Images\n",
      "Processed  8000  Images\n",
      "Processed  9000  Images\n",
      "Processed  10000  Images\n",
      "Processed  11000  Images\n",
      "Processed  12000  Images\n",
      "Time Taken for Processing:  0:25:03.874079\n"
     ]
    }
   ],
   "source": [
    "synth_val_accuracy,synth_val_letter_acc,synth_val_letter_cnt,synth_val_mis_match=test_data_output_Prediction(model,val_img_names,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output Accuracy:  60.21666666666666  %\n",
      "Model Output Letter Accuracy:  80.75212040578747  %\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Output Accuracy: \",(synth_val_accuracy/len(val_labels))*100, \" %\")\n",
    "print(\"Model Output Letter Accuracy: \",(synth_val_letter_acc/synth_val_letter_cnt)*100, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_val_mis_match_dict=Counter(synth_val_mis_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1 Validation Data Prediction Analysis upto 4 Character Mis-Matches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_match_cnts_1=[]\n",
    "for i in range(5):\n",
    "    mis_match_cnts_1.append(model_1_val_mis_match_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mis_match_character_analysis_plot(mis_match_counts,num_values):\n",
    "    \"\"\"\n",
    "    Takes mis-match counts upto 4 characters of the predicted output, total number of values and\n",
    "    plots the percentage of number of mis-match characters between predicted and actual labels\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    indices=np.arange(len(mis_match_counts))\n",
    "    counts=np.array(mis_match_counts)\n",
    "    percent=(counts/num_values)*100\n",
    "    plt.bar(indices,percent)\n",
    "    plt.xlabel('Number of Mis-Match Characters',fontsize=10)\n",
    "    plt.ylabel('Percentages',fontsize=10)\n",
    "    plt.title('Percentages of Number of Mis-Match Characters',fontsize=12)\n",
    "    plt.xticks(indices,indices)\n",
    "    plt.show()\n",
    "    for i in range(len(indices)):\n",
    "        print(i,\" Mis-Match Characters Percentage: \",np.round(percent[i],2),\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAGDCAYAAAD6aR7qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3debytZV338c+Xc1BmUDkS80FABMkRcSojNCdU8HkkB1Q0ihQHLM0pn8RKg8rEzFIUFZwIpVKLnBA0zERAlEnFkEkZVSZFxt/zx31tWGz23mcdPOvag5/367VeZ93z777vtfb6nuu+1r1SVUiSJGny1prvAiRJkn5VGLwkSZI6MXhJkiR1YvCSJEnqxOAlSZLUicFLkiSpE4OXpDtJ8swkFye5PslDF0A9JyX5/Xna9rpJPpPkmiSf+CXW85tJvrsma5uUJCuTVJLla2h9eyS5ZE2sS1oKDF5atJJckOSGFhAuT/LBJBvMd12jWo1PmO86VtPfAi+vqg2q6pvTJ7YP5TOTrDUy7i+TfKhnkZ08C9gMuE9V7Tt9YpJD2vF45bTxr2rjDwGoqv+qqp1Wd+MjIej0aeM3TXJTkgvGXM+Lkpy8utsfV5Ldkxyf5OokP0lySpIXT2p7d8eaDpTS3WXw0mL39KraAHgY8AjgTau7Av8Q38W2wNmrmGcL4DkdalljMljdv3nbAt+rqlvmmOd7wP7Txr2wjV9T1k+y68jw84AfrMH1321JHg18CfgysANwH+ClwFMmsK15e6/6d0JrisFLS0JV/RD4T2BXgCQbJzkyyaVJfthaZJa1aS9K8tUk70jyE+CQNv4Pkpyb5Lok5yR5WBu/RZLjklyZ5AejrRutxePYJEe35c5Oslub9mFgG+AzrVXutW38J5Jc1i5ffSXJA0fWd592aevaJN9odZ88Mv0BSb7QWhW+m+R3R6Y9tdV9Xdvn18x0rJKsleRNSS5MckWrfeMk90xyPbAM+FaS/53jkP818JaZPoxmurQ02vLXjtknknyk1XpmkvsneUOr5+IkT5y22u1bK8o1ST6V5N4j635Ukv9urS3fSrLHyLSTkrw1yVeBnwP3m6Hendt8V7fz94w2/i3AnwHPbufvgFmOxTeA9abOY/t33TZ+xmOS5HXtHF3XzuPjZ1n3lA9z53D3QuDoafvx+iT/O/L6febU/gHvAR7d9uPqNn7dJG9vr4NrkpycZN2RVe6X5KIkVyX50zlq+xvgqKo6rKquqsFpVfW7ozMleXU7v5dmpDUsyV5Jvtle8xentRK2aVOtVAckuYgh4K3qPTTbfn2lzXJ1Ow6PbvP/Xob3/U+TfC7JtiPrqiQvS3IecF4G72j7cU2Sb+fOgVhatary4WNRPoALgCe051sztNL8RRv+N+C9wPrAfYFTgD9s014E3AK8AljO8CG5L/BDhlazMPzPfVuG/5ycxvABfA+GD+7zgSe1dR0C/AJ4KkNg+Svgf2aqcWTc7wEbAvcEDgfOGJl2THusB+wCXAyc3Kat34Zf3Op+GHAV8MA2/VLgN9vzewEPm+W4/R7w/bYvGwD/Anx4ZHoBO8xx3AvYsR2X32/j/hL4UHu+B3DJHOdq6pg9qe3H0QytN38KrA38AfCDkWVPaudm13YMjgM+0qZtCfy4Hf+1gN9pwytGlr0IeGDb1trT6lq7HYs3tvO7J3AdsNNIrR+Z41gcAnykLX9YG/fXwBva+EOmHxNgp3Yet2jDK4HtZ1n/yna8V7ZllgE7A98FngBcMDLvvgwtkWsBzwZ+Bmw+8po/edq6392Oz5ZtvY9heE1ObfN9DO+NBwM3AjvPUN96wK3Ab89xjPZgeL/9eTveT2UIwfcamf7rre4HAZcD+0zb/6PbuV93jPfQqvZr+ci8+7Tzv3N7fbwJ+O9pr/UvAPdux+JJDK/7TRj+Tuw8dYx9+Bj3Me8F+PBxdx8MH+bXA1cDFwL/2P44btY+KNYdmfe5wInt+YuAi6at63PAwTNs45EzzPsG4IPt+SHAF0em7QLcMK3GJ8yxD5u0P+4btw+Jm2kf+m36X3JH8Ho28F/Tln8v8Ob2/CLgD4GNVnHcTgAOGhneqW13eRseJ3jt0D5AL2ofaqsbvL4wMu3p7Twua8Mbtm1s0oZPAg6ddoxvasfrdYyExpFzuf/Isn8+x778JnAZsNbIuI9zR2A6hPGC1zbtWKzd/t2a2YPXDsAVDMFp7dnW3eZd2Y7FcuCLDB/8hzKE1DsFrxmWPQPYe+Q1f/LItLWAG4AHz7HNrUbGnQI8Z4Z5t2zzPmCOOvZo2xoNPFcAj5pl/sOBd0yr5X5jvofG2a/ROv4TOGDacfk5sO3Ia33Pkel7MlxCftToa8aHj9V5eKlRi90+VbVJVW1bVQdV1Q0MLVVrA5e2y0dXMwSU+44sd/G09WwNzHRpbVtgi6n1tHW9kSHcTbls5PnPgXVmugQHkGRZkkPbJaFrGQIJwKbACoYP2NHaRp9vCzxyWi37Ab/Wpv9fhjB0YZIvT11KmcEWDEF1yoVtu5vNPPvMqup4hpBx4Oos11w+8vwG4KqqunVkGIbWuCmjx+FChvO7KcMx2XfaMfkNYPNZlp1uC+Diqrpt2vq3HHtPgKq6iKHl5G3AeVU16zar6vvAqxhC2xVJjkmyBUC7BDb12GbaokczBKjnMoS6O0nywiRnjByHXRmO0Uw2BdZh5tf8lOmv65m+uPJT4DbufLxn8uO6cz+529eX5JFJTsxwKf8a4CUz1H378VzFe2ic/Rq1LfDOkWP2E4aWrNHzf/u2q+pLwD8wtKpdnuSIJBuNuS0JsI+XlqaLGVq8Nm2hbJOq2qiqHjgyT82wzPazrOsHI+vZpKo2rKqnjlnL9O08D9ibobViY4b/hcPwx/5KhksyW43Mv/W0Wr48rZYNquqlAFX1jaramyFg/htw7Cw1/YjhA2fKNm27l888+5zexND6st7IuJ+NDmfoW7fibqx71Ohx2Iahhe4qhmPy4WnHZP2qOnRk/unnYNSPgK1z50732zBc2lxdRwOvZlrfq5lU1ceq6jcYzkMBh7XxG4w8Lpq22HHAXsD5VTUanGn9kt4HvJzhG5ibAGcxvK7grsfgKobLvTO95sdWVT8HvsYQ+u+ujwGfBrauqo0Z+qNl2jyj9c/1Hpprv2Z6HVzM0AVh9PWzblX992zLVdXfV9XDGS5f3x/4k1XvonQHg5eWnKq6FPg88PYkG2XoTL59kt+aY7H3A69J8vDWgXaH9mF2CnBt6wy9bvvf9q5JHjFmOZdz5w7dGzKEwh8zhJO3jdR9K0N/q0OSrJfkAQydqKf8O3D/JC9IsnZ7PCJD5/B7JNkvycZVdTNwLUPfm5l8HPijJNtluP3G24B/rrm/uTejqjoJOJM7d/z+HkOr315J1mYIZ/dc3XVP8/wkuyRZj6Gv0Cfb8foI8PQkT2rnZp0MHdm3mnt1t/s6Q1B8bTueezBc+jzmbtT4z8ATmT3wApBkpyR7JrknQ0i4gdnP1e2q6mcMl7pmuqfZ+gwB4cq2jRfTvmjSXA5sleQebV23AR8A/i7Dl0eWJXl0q2l1vRZ4UZI/SXKftv0HJxn3GG4I/KSqfpFkd4Zgtar5Z3sPzbVfVzK0zo2+H98DvCF3fDFi4yR3uW3IlPZ+e2R7Xf+M4fyt8txJowxeWqpeyNBZ+hyGyyGfZI7LIVX1CeCtDP/7vo6hxeje7cP96cBDGDqAX8UQ0jYes46/At7ULmW8hqE15EKGFpVzgP+ZNv/L27ovY/gm28cZPmSoqusYPtifw9BScxlDS8nUh+ULgAva5ZeXAM+fpaYPtHV/pe3TLxi+aHB3vYmh8zGtzmuAgxiO0w8ZPqB+2Rtofhj4EMM+rwO8sm3rYobWjzcyfLBezNACMdbftqq6CXgGw60PrmLoJ/jCqvrO6hZYVTdU1Rfb5e653JOhn9ZVbX/u2+ofZxunVtVdLqNV1TnA2xlany5n6Kz+1ZFZvsTw5ZPLklzVxr2GITR/g+ES22Hcjc+E1jq0Z3ucn+GbwkcAx4+5ioOAP09yHcOXWOYMrqz6PTTjfrXWubcCX23vx0dV1b+26ce0981ZzH0bjI0YWhZ/2mr4McN976SxpWquVnhJ8ynJYcCvVdX+q5xZkrTg2eIlLSAZ7tP1oHa5c3fgAOBf57suSdKa4Z14pYVlQ4bLi1swfOX+7cCn5rUiSdIa46VGSZKkTrzUKEmS1InBS5IkqZNF0cdr0003rZUrV853GZIkSat02mmnXVVVM944elEEr5UrV3LqqafOdxmSJEmrlOTC2aZ5qVGSJKkTg5ckSVInBi9JkqRODF6SJEmdGLwkSZI6MXhJkiR1YvCSJEnqxOAlSZLUicFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOlk+3wUsFCtf/x/zXcKSc8Ghe813CZIkLSi2eEmSJHVi8JIkSerE4CVJktSJwUuSJKkTg5ckSVInBi9JkqRODF6SJEmdGLwkSZI6MXhJkiR1YvCSJEnqxOAlSZLUicFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOjF4SZIkdWLwkiRJ6sTgJUmS1MlEg1eSP0pydpKzknw8yTpJtkvy9STnJfnnJPeYZA2SJEkLxcSCV5ItgVcCu1XVrsAy4DnAYcA7qmpH4KfAAZOqQZIkaSGZ9KXG5cC6SZYD6wGXAnsCn2zTjwL2mXANkiRJC8LEgldV/RD4W+AihsB1DXAacHVV3dJmuwTYclI1SJIkLSSTvNR4L2BvYDtgC2B94CkzzFqzLH9gklOTnHrllVdOqkxJkqRuJnmp8QnAD6rqyqq6GfgX4DHAJu3SI8BWwI9mWriqjqiq3apqtxUrVkywTEmSpD4mGbwuAh6VZL0kAR4PnAOcCDyrzbM/8KkJ1iBJkrRgTLKP19cZOtGfDpzZtnUE8Drgj5N8H7gPcOSkapAkSVpIlq96lruvqt4MvHna6POB3Se5XUmSpIXIO9dLkiR1YvCSJEnqxOAlSZLUicFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOjF4SZIkdWLwkiRJ6sTgJUmS1InBS5IkqRODlyRJUicGL0mSpE4MXpIkSZ0YvCRJkjoxeEmSJHVi8JIkSerE4CVJktSJwUuSJKkTg5ckSVInBi9JkqRODF6SJEmdGLwkSZI6MXhJkiR1YvCSJEnqxOAlSZLUicFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOjF4SZIkdWLwkiRJ6sTgJUmS1InBS5IkqRODlyRJUicGL0mSpE4MXpIkSZ0YvCRJkjoxeEmSJHVi8JIkSerE4CVJktSJwUuSJKkTg5ckSVInBi9JkqRODF6SJEmdGLwkSZI6MXhJkiR1YvCSJEnqxOAlSZLUicFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOjF4SZIkdWLwkiRJ6sTgJUmS1InBS5IkqRODlyRJUicGL0mSpE4MXpIkSZ1MNHgl2STJJ5N8J8m5SR6d5N5JvpDkvPbvvSZZgyRJ0kIx6RavdwKfraoHAA8GzgVeD5xQVTsCJ7RhSZKkJW9iwSvJRsDjgCMBquqmqroa2Bs4qs12FLDPpGqQJElaSCbZ4nU/4Ergg0m+meT9SdYHNquqSwHav/edYA2SJEkLxiSD13LgYcA/VdVDgZ+xGpcVkxyY5NQkp1555ZWTqlGSJKmbSQavS4BLqurrbfiTDEHs8iSbA7R/r5hp4ao6oqp2q6rdVqxYMcEyJUmS+phY8Kqqy4CLk+zURj0eOAf4NLB/G7c/8KlJ1SBJkrSQLJ/w+l8BfDTJPYDzgRczhL1jkxwAXATsO+EaJEmSFoSJBq+qOgPYbYZJj5/kdiVJkhYi71wvSZLUicFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOjF4SZIkdWLwkiRJ6sTgJUmS1InBS5IkqRODlyRJUicGL0mSpE4MXpIkSZ0YvCRJkjoxeEmSJHVi8JIkSerE4CVJktSJwUuSJKmTsYJXku2T3LM93yPJK5NsMtnSJEmSlpZxW7yOA25NsgNwJLAd8LGJVSVJkrQEjRu8bquqW4BnAodX1R8Bm0+uLEmSpKVn3OB1c5LnAvsD/97GrT2ZkiRJkpamcYPXi4FHA2+tqh8k2Q74yOTKkiRJWnqWjzNTVZ2T5HXANm34B8ChkyxMkiRpqRn3W41PB84APtuGH5Lk05MsTJIkaakZ91LjIcDuwNUAVXUGwzcbJUmSNKZxg9ctVXXNtHG1pouRJElaysbq4wWcleR5wLIkOwKvBP57cmVJkiQtPeO2eL0CeCBwI/Bx4FrgVZMqSpIkaSka91uNPwf+tD0kSZJ0N4wVvJJ8hrv26boGOBV4b1X9Yk0XJkmStNSMe6nxfOB64H3tcS1wOXD/NixJkqRVGLdz/UOr6nEjw59J8pWqelySsydRmCRJ0lIzbovXiiTbTA2055u2wZvWeFWSJElL0LgtXq8GTk7yv0AYbp56UJL1gaMmVZwkSdJSMu63Go9v9+96AEPw+s5Ih/rDJ1WcJEnSUjJuixfAjsBOwDrAg5JQVUdPpixJkqSlZ9zbSbwZ2APYBTgeeApwMmDwkiRJGtO4neufBTweuKyqXgw8GLjnxKqSJElagsYNXjdU1W3ALUk2Aq4A7je5siRJkpaecft4nZpkE4abpZ7GcDPVUyZWlSRJ0hI07rcaD2pP35Pks8BGVfXtyZUlSZK09Ix1qTHJCVPPq+qCqvr26DhJkiSt2pwtXknWAdYDNk1yL4Z7eAFsBGwx4dokSZKWlFVdavxD4FUMIes07ghe1wLvnmBdkiRJS86cwauq3gm8M8krqupdnWqSJElaksbtXP+uJI8BVo4u453rJUmSxjfunes/DGwPnAHc2kYX3rlekiRpbOPex2s3YJeqqkkWI0mStJSNe+f6s4Bfm2QhkiRJS924LV6bAuckOQW4cWpkVT1jIlVJkiQtQeMGr0MmWYQkSdKvgnG/1fjlJNsCO1bVF5OsByybbGmSJElLy7g/GfQHwCeB97ZRWwL/NqmiJEmSlqJxO9e/DHgswx3rqarzgPtOqihJkqSlaNzgdWNV3TQ1kGQ5w328JEmSNKZxg9eXk7wRWDfJ7wCfAD4zubIkSZKWnnGD1+uBK4EzGX44+3jgTZMqSpIkaSka93YS6wIfqKr3ASRZ1sb9fFKFSZIkLTXjtnidwBC0pqwLfHHNlyNJkrR0jRu81qmq66cG2vP1JlOSJEnS0jRu8PpZkodNDSR5OHDDZEqSJElamsbt43Uw8IkkP2rDmwPPnkxJkiRJS9Mqg1eStYB7AA8AdgICfKeqbh5nA60j/qnAD6vqaUm2A44B7g2cDrxg9B5hkiRJS9UqLzVW1W3A26vq5qo6q6rOHDd0NQcD544MHwa8o6p2BH4KHLBaFUuSJC1S4/bx+nyS/5skq7PyJFsBewHvb8MB9mT43UeAo4B9VmedkiRJi9W4fbz+GFgfuDXJDQyXG6uqNlrFcocDrwU2bMP3Aa6uqlva8CUMP7h9F0kOBA4E2GabbcYsU5IkaeEaq8WrqjasqrWqau2q2qgNzxm6kjwNuKKqThsdPdPqZ9nmEVW1W1XttmLFinHKlCRJWtDGavFqlwj3A7arqr9IsjWweVWdMsdijwWekeSpwDrARgwtYJskWd5avbYCfjTHOiRJkpaMcft4/SPwaOB5bfh64N1zLVBVb6iqrapqJfAc4EtVtR9wIvCsNtv+wKdWt2hJkqTFaNzg9ciqehnwC4Cq+inDLSbujtcBf5zk+wx9vo68m+uRJElaVMbtXH9zux9XASRZAdw27kaq6iTgpPb8fGD31apSkiRpCRi3xevvgX8F7pvkrcDJwNsmVpUkSdISNFaLV1V9NMlpwOMZvpm4T1Wdu4rFJEmSNGLO4JVkHeAlwA7AmcB7R+7BJUmSpNWwqkuNRwG7MYSupwB/O/GKJEmSlqhVXWrcpap+HSDJkcBc9+2SJEnSHFbV4nX7j2F7iVGSJOmXs6oWrwcnubY9D7BuGx73txolSZLUzBm8qmpZr0IkSZKWunHv4yVJkqRfksFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOjF4SZIkdWLwkiRJ6sTgJUmS1InBS5IkqRODlyRJUicGL0mSpE4MXpIkSZ0YvCRJkjoxeEmSJHVi8JIkSerE4CVJktSJwUuSJKkTg5ckSVInBi9JkqRODF6SJEmdGLwkSZI6MXhJkiR1YvCSJEnqxOAlSZLUicFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOjF4SZIkdWLwkiRJ6sTgJUmS1InBS5IkqRODlyRJUicGL0mSpE4MXpIkSZ0YvCRJkjoxeEmSJHVi8JIkSerE4CVJktSJwUuSJKkTg5ckSVInBi9JkqRODF6SJEmdGLwkSZI6MXhJkiR1YvCSJEnqxOAlSZLUicFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOjF4SZIkdTKx4JVk6yQnJjk3ydlJDm7j753kC0nOa//ea1I1SJIkLSSTbPG6BXh1Ve0MPAp4WZJdgNcDJ1TVjsAJbViSJGnJm1jwqqpLq+r09vw64FxgS2Bv4Kg221HAPpOqQZIkaSHp0scryUrgocDXgc2q6lIYwhlw31mWOTDJqUlOvfLKK3uUKUmSNFETD15JNgCOA15VVdeOu1xVHVFVu1XVbitWrJhcgZIkSZ1MNHglWZshdH20qv6ljb48yeZt+ubAFZOsQZIkaaGY5LcaAxwJnFtVfzcy6dPA/u35/sCnJlWDJEnSQrJ8gut+LPAC4MwkZ7RxbwQOBY5NcgBwEbDvBGuQJElaMCYWvKrqZCCzTH78pLYrSZK0UHnnekmSpE4MXpIkSZ0YvCRJkjoxeEmSJHVi8JIkSerE4CVJktSJwUuSJKkTg5ckSVInBi9JkqRODF6SJEmdGLwkSZI6MXhJkiR1YvCSJEnqxOAlSZLUicFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOjF4SZIkdWLwkiRJ6sTgJUmS1InBS5IkqRODlyRJUicGL0mSpE4MXpIkSZ0YvCRJkjoxeEmSJHVi8JIkSerE4CVJktSJwUuSJKkTg5ckSVInBi9JkqRODF6SJEmdGLwkSZI6MXhJkiR1YvCSJEnqxOAlSZLUicFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOjF4SZIkdWLwkiRJ6mT5fBcgrY6Vr/+P+S5hSbng0L3muwRJ+pVii5ckSVInBi9JkqRODF6SJEmdGLwkSZI6MXhJkiR1YvCSJEnqxOAlSZLUicFLkiSpE4OXJElSJwYvSZKkTgxekiRJnRi8JEmSOvFHsiWtcf6Y+Zrlj5lLS4ctXpIkSZ0YvCRJkjoxeEmSJHUyL328kjwZeCewDHh/VR06H3VI0q8q++GteZPoi+d5WrMWQn/J7i1eSZYB7waeAuwCPDfJLr3rkCRJ6m0+LjXuDny/qs6vqpuAY4C956EOSZKkruYjeG0JXDwyfEkbJ0mStKSlqvpuMNkXeFJV/X4bfgGwe1W9Ytp8BwIHtsGdgO92LXTh2hS4ar6L0Cp5nhYHz9Pi4Hla+DxHd7ZtVa2YacJ8dK6/BNh6ZHgr4EfTZ6qqI4AjehW1WCQ5tap2m+86NDfP0+LgeVocPE8Ln+dofPNxqfEbwI5JtktyD+A5wKfnoQ5JkqSuurd4VdUtSV4OfI7hdhIfqKqze9chSZLU27zcx6uqjgeOn49tLwFefl0cPE+Lg+dpcfA8LXyeozF171wvSZL0q8qfDJIkSerE4LWIJHlyku8m+X6S1893PbqrJB9IckWSs+a7Fs0sydZJTkxybpKzkxw83zXprpKsk+SUJN9q5+kt812TZpdkWZJvJvn3+a5loTN4LRL+1NKi8SHgyfNdhOZ0C/DqqtoZeBTwMt9LC9KNwJ5V9WDgIcCTkzxqnmvS7A4Gzp3vIhYDg9fi4U8tLQJV9RXgJ/Ndh2ZXVZdW1ent+XUMHxb+esYCU4Pr2+Da7WGn5AUoyVbAXsD757uWxcDgtXj4U0vSGpZkJfBQ4OvzW4lm0i5fnQFcAXyhqjxPC9PhwGuB2+a7kMXA4LV4ZIZx/u9PupuSbAAcB7yqqq6d73p0V1V1a1U9hOEXTnZPsut816Q7S/I04IqqOm2+a1ksDF6Lx1g/tSRp1ZKszRC6PlpV/zLf9WhuVXU1cBL2n1yIHgs8I8kFDF1g9kzykfktaWEzeC0e/tSStAYkCXAkcG5V/d1816OZJVmRZJP2fF3gCcB35rcqTVdVb6iqrapqJcPn0peq6vnzXNaCZvBaJKrqFmDqp5bOBY71p5YWniQfB74G7JTkkiQHzHdNuovHAi9g+J/5Ge3x1PkuSnexOXBikm8z/MfzC1XlrQq06HnnekmSpE5s8ZIkSerE4CVJktSJwUuSJKkTg5ckSVInBi9JkqRODF7SIpWkkrx9ZPg1SQ5ZQ+v+UJJnrYl1rWI7+yY5N8mJ08avbPv3FyPjNk1yc5J/aMMvSfLC1djWSUkuavfxmhr3b0muX8VymyQ5aMz17zbGfPdPcnyS77d9PzbJZkleNLVvPYy7X5LWLIOXtHjdCPyfJJvOdyGjkixbjdkPAA6qqt+eYdr5wNNGhvcFbr93XVW9p6qOXs3yrma4jxft5pybj7HMJsAaCShJ1gH+A/inqtqhqnYG/glYsQbWvTrHHe7GfmXg54b0S/ANJC1etwBHAH80fcL0FqupVp0keyT5cmtl+V6SQ5Psl+SUJGcm2X5kNU9I8l9tvqe15Zcl+Zsk30jy7SR/OLLeE5N8DDhzhnqe29Z/VpLD2rg/A34DeE+Sv5lh/24Azh1pRXo2cOzIOg9J8pr2/JVJzmk1HTPHMTuG4e7aAP8HuP3ngpJskOSEJKe3Wvdukw4Ftm83Wv2bNu9r2zzfSnLoyPr3bcfye0l+c4btPw/4WlV9ZmpEVZ1YVWe1wS2SfDbJeUn+eqS2f0pyapKzk7xlZPwFSW6whfsAAATbSURBVP4syclt23/Qzs23khyXZL0232ZJ/rWN/1aSx8yyX38ycm7f0satbC1z/wicDmzdXl9ntWNwl9efpNktn+8CJP1S3g18e/RDegwPBnYGfsLQqvT+qto9ycHAK4BXtflWAr8FbM9wB/EdgBcC11TVI5LcE/hqks+3+XcHdq2qH4xuLMkWwGHAw4GfAp9Psk9V/XmSPYHXVNWps9R6DPCcJJcBtzL8PukWM8z3emC7qrqxtWTN5gTgfa116DnAgcD/a9N+ATyzqq5trYj/k+TTbd27th9rJslTgH2AR1bVz5Pce2T9y9uxfCrwZoafuRm1KzDXjwk/BHgoQ2vmd5O8q6ouBv60qn7S6j4hyYOq6ttTdVfVb7Ta7lNV72vP/5KhRfFdwN8DX66qZ7Z1bDDDfj0R2JHhPAb4dJLHARcBOwEvrqqDkjwc2LKqdm3LzXW8JU1ji5e0iFXVtcDRwCtXY7FvVNWlVXUj8L/AVHA6kyFsTTm2qm6rqvMYAtoDgCcCL0xyBvB14D4MH9YAp0wPXc0jgJOq6sr201cfBR43Zq2fBX4HeC7wz3PM923go0mez9ASOJtbgZMZWs/WraoLRqYFeFuGn6j5IrAlsNkM63gC8MGq+jlAVf1kZNpUC9pp3PlYjuuEqrqmqn4BnANs28b/bpLTgW8CDwR2GVlm9Ljs2lopzwT2a/MC7MlwSZOqurWqrplh209sj28ytGw9gDvO7YVV9T/t+fnA/ZK8K8mTgWvvxn5Kv7IMXtLidzhDy8b6I+Nuob2/kwS4x8i0G0ee3zYyfBt3bgWf/ntixRBOXlFVD2mP7apqKrj9bJb6Msv4VaqqmxhCzKuB4+aYdS+G1r+HA6clWZ7kc+0y2vunzXsMQyvQsdPG78fQ1+rhrRXocmCdGbYV7npspkwdy1uZ+YrC2a3G2Yyem1uB5Um2A14DPL6qHsTQR2y0rtHj/iHg5VX168BbZql/NgH+auTc7lBVR07fRlX9lKHV9CTgZcD04ytpDgYvaZFrLS7HMoSvKRdwxwf83sDad2PV+yZZq/X7uh/wXYYfaX9pkrXh9m/orT/XShhaxn4rw7cSlzG0Xn15Nep4O/C6qvrxTBMzdPbeuqpOBF7L0Gl8g6p6UgsQvz9tkf8C/gr4+LTxGwNXVNXNSX6bO1qbrgM2HJnv88DvjfSfujfj+xjwmCR7jdT/5CS/PscyGzEEn2uSbAY8ZY55NwQubednv5HxJwAvbdtblmQj7rpfn2PYrw3afFsmue/0DbTLsGtV1XEMl2kfNkc9kqaxj5e0NLwdePnI8PuATyU5heFDd7bWqLl8lyEgbQa8pKp+0VqPVgKnt5a0Kxn6O82qqi5N8gbgRIZWleOr6lPjFlFVZzPybcYZLAM+kmTjtv53VNXVc6yvgL+dYdJHgc8kORU4A/hOm//HSb6a5CzgP6vqT5I8BDg1yU3A8cAbx9yXGzJ8UeHwJIcDNzNcJj14jmW+leSbDMfgfOCrc2zi/zEE3QsZLh1PBauDgSOSHMDQkvbSqvraDPu1M/C14dRyPfD8Nv+oLYEP5o5vN75hnH2XNMjwN0iSJEmT5qVGSZKkTgxekiRJnRi8JEmSOjF4SZIkdWLwkiRJ6sTgJUmS1InBS5IkqRODlyRJUif/H53iO62swjmXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Mis-Match Characters Percentage:  86.58  %\n",
      "1  Mis-Match Characters Percentage:  5.78  %\n",
      "2  Mis-Match Characters Percentage:  2.15  %\n",
      "3  Mis-Match Characters Percentage:  1.18  %\n",
      "4  Mis-Match Characters Percentage:  1.07  %\n"
     ]
    }
   ],
   "source": [
    "mis_match_character_analysis_plot(mis_match_cnts_1,12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synth Text Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_names=test_data['ImageName'].values\n",
    "test_labels=test_data['Labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  1000  Images\n",
      "Processed  2000  Images\n",
      "Processed  3000  Images\n",
      "Processed  4000  Images\n",
      "Processed  5000  Images\n",
      "Processed  6000  Images\n",
      "Processed  7000  Images\n",
      "Processed  8000  Images\n",
      "Processed  9000  Images\n",
      "Processed  10000  Images\n",
      "Processed  11000  Images\n",
      "Processed  12000  Images\n",
      "Processed  13000  Images\n",
      "Processed  14000  Images\n",
      "Processed  15000  Images\n",
      "Time Taken for Processing:  0:21:50.478253\n"
     ]
    }
   ],
   "source": [
    "synth_test_accuracy,synth_test_letter_acc,synth_test_letter_cnt,synth_test_mis_match=test_data_output_Prediction(model,test_img_names,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output Accuracy:  87.08666666666667  %\n",
      "Model Output Letter Accuracy:  94.48166697375254  %\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Output Accuracy: \",(synth_test_accuracy/len(test_labels))*100, \" %\")\n",
    "print(\"Model Output Letter Accuracy: \",(synth_test_letter_acc/synth_test_letter_cnt)*100, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_test_mis_match_dict=Counter(synth_test_mis_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1 Test Data Prediction Analysis upto 4 Character Mis-Matches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_match_cnts_2=[]\n",
    "for i in range(5):\n",
    "    mis_match_cnts_2.append(model_1_test_mis_match_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Mis-Match Characters Percentage:  87.19  %\n",
      "1  Mis-Match Characters Percentage:  5.85  %\n",
      "2  Mis-Match Characters Percentage:  2.1  %\n",
      "3  Mis-Match Characters Percentage:  1.17  %\n",
      "4  Mis-Match Characters Percentage:  0.91  %\n"
     ]
    }
   ],
   "source": [
    "mis_match_character_analysis_plot(mis_match_cnts_2,15000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
